# RNN的梯度弥散和梯度爆炸问题

对于Wh^k
Whh > 1,趋近于无穷大：梯度爆炸
Whh < 1,趋近于0：梯度弥散

![image-20230826195814582](RNN的梯度弥散和梯度爆炸问题/image-20230826195814582-16930511008751.png)

## 如何解决梯度爆炸

在每一次计算的时候加入钳制clip
如果梯度大于某个阈值，将梯度设定为1（方向不变）

![image-20230826195956047](RNN的梯度弥散和梯度爆炸问题/image-20230826195956047-16930511979903.png)

## 如何解决梯度弥散——LSTM

简单的RNN只有短期的记忆，LSTM延长记忆，取得了更长的记忆

RNN：

<img src="RNN的梯度弥散和梯度爆炸问题/image-20230826201429514-16930520724655-16930520813867.png" alt="image-20230826201429514" style="zoom:50%;" />

LSTM：

<img src="RNN的梯度弥散和梯度爆炸问题/image-20230826201803571-16930522848329.png" alt="image-20230826201803571" style="zoom:67%;" />

设计了三道门，控制ht，xt和输出的信息

![image-20230826202225692](RNN的梯度弥散和梯度爆炸问题/image-20230826202225692-169305254747011.png)

过去的记忆通过C_t-1遗忘，与C~融合形成新的记忆

### 遗忘门：

<img src="RNN的梯度弥散和梯度爆炸问题/image-20230826202338268-169305261963813.png" alt="image-20230826202338268" style="zoom:80%;" />

### 输入门：

<img src="RNN的梯度弥散和梯度爆炸问题/image-20230826202538454-169305273963115.png" alt="image-20230826202538454" style="zoom:80%;" />

然后更新这个

<img src="RNN的梯度弥散和梯度爆炸问题/image-20230826202822039-169305290623417.png" alt="image-20230826202822039" style="zoom:80%;" />

### 输出门：

<img src="RNN的梯度弥散和梯度爆炸问题/image-20230826203023368-169305302516019.png" alt="image-20230826203023368" style="zoom:80%;" />